{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ML Model Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch.nn.functional as F\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For GAN Implementation \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# For AWS \n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "# URL to the dataset\n",
    "url = \"https://www.kaggle.com/api/v1/datasets/download/rupakroy/online-payments-fraud-detection-dataset\"\n",
    "\n",
    "# Destination path to save the file\n",
    "destination = 'data_set.zip'\n",
    "\n",
    "# Download the file\n",
    "urllib.request.urlretrieve(url, destination)\n",
    "\n",
    "print(f\"File downloaded successfully and saved as {destination}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!unzip data_set.zip -d data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/Data_Set.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of rows and columns:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#describing the data\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling the NULL Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df.isnull().values.any():\n",
    "    print('There are some missing values in this dataset\\n')\n",
    "    df.dropna(inplace=True)\n",
    "    print('Shape : ', df.shape) \n",
    "else:\n",
    "    print('GREAT, There is no missing values in this dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA of categories of the \"type\" feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature 'type' contains the classes which is essential for the model to classify the data into the right class. So Label Encoding is applied on 'type' feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Labels in feature type :- \",df['type'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The counts of each category :- \",df['type'].value_counts())\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title('type vs counts')\n",
    "sns.countplot(data=df, x='type', color='skyblue')\n",
    "plt.xlabel('Type')\n",
    "plt.ylabel('Counts')\n",
    "plt.grid(axis='y', alpha=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Label Encoding</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['type'].replace({'CASH_OUT':0, 'PAYMENT':1, 'CASH_IN':2, 'TRANSFER':3, 'DEBIT':4}, inplace=True)\n",
    "df['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop unnecessary columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column mentioned below were dropped as these features don't add any information during the training phase of the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['nameOrig', 'nameDest'], axis=1, inplace=True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA of categories of the \"isFraud\" feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The class in 'isFraud' feature :- \",df['isFraud'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class 0 represents normal transactions whereas 1 represents fraudulent transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['isFraud'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above summary we can see that the data is highly imbalanced, where 0 represents a normal transaction and 1 represents an anomaly.\n",
    "As we have more than 6.3 million records with imbalanced data, we don't want to handle too much data.So, we are limiting the data that we access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all fraud cases (class 1)\n",
    "fraud_df = df[df['isFraud'] == 1]\n",
    "\n",
    "# Randomly sample 10000 non-fraud cases (class 0)\n",
    "non_fraud_df = df[df['isFraud'] == 0].sample(n=10000, random_state=42)\n",
    "\n",
    "# Combine both dataframes\n",
    "df = pd.concat([fraud_df, non_fraud_df])\n",
    "\n",
    "# Shuffle the combined dataframe\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['isFraud'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_columns = ['amount', 'oldbalanceOrg', 'newbalanceDest']\n",
    "\n",
    "# Standardizing the numerical columns for consistent scaling\n",
    "scaler = StandardScaler()\n",
    "df[scaling_columns] = scaler.fit_transform(df[scaling_columns])\n",
    "\n",
    "# Check the transformed data\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN implementation for Class Immbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a GAN?\n",
    "A Generative Adversarial Network (GAN) consists of two neural networks:\n",
    "\n",
    "    Generator: Creates fake (synthetic) data that resembles the real data.\n",
    "    Discriminator: Tries to distinguish between real data (from the dataset) and fake data (from the generator).\n",
    "\n",
    "They compete in a \"game\" where:\n",
    "\n",
    "    The Generator improves at creating realistic data.\n",
    "    The Discriminator gets better at identifying fake data.\n",
    "\n",
    "Over time, the generator learns to produce data indistinguishable from the real dataset.\n",
    "\n",
    "Leaky ReLU is a powerful activation function that improves upon the traditional ReLU by allowing small negative values to pass through. \n",
    "\n",
    "Optimizer is like a smart coach that helps neural networks learn better by adjusting their parameters (weights and biases) to minimize errors during training.\n",
    "\n",
    "Adam optimizer is a popular and efficient type of optimizer that:\n",
    "\n",
    "    Adapts the learning rate automatically for each parameter\n",
    "    Combines the best aspects of two other optimization methods (RMSprop and Momentum)\n",
    "    Works well for most deep learning tasks without requiring much tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),  # First layer\n",
    "            nn.LeakyReLU(0.2),  # Activation function\n",
    "            nn.Linear(128, 256),  # Second layer\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, output_dim),  # Output layer\n",
    "            nn.Tanh()  # Ensures the generated output is in the range [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "# Define the Discriminator class\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),  # Input layer\n",
    "            nn.LeakyReLU(0.2),  # Activation function\n",
    "            nn.Dropout(0.3),  # Regularization to prevent overfitting\n",
    "            nn.Linear(256, 128),  # Hidden layer\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 1),  # Output layer\n",
    "            nn.Sigmoid()  # Ensures the output is between 0 and 1 for binary classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only fraud data\n",
    "fraud_data = df[df['isFraud'] == 1]\n",
    "\n",
    "# Hyperparameters\n",
    "latent_dim = 100  # Size of the random noise input for the generator\n",
    "input_dim = df.shape[1]  # The number of features in the data\n",
    "\n",
    "# Initialize the generator and discriminator\n",
    "generator = Generator(latent_dim, input_dim)\n",
    "discriminator = Discriminator(input_dim)\n",
    "\n",
    "# Loss function and optimizers\n",
    "adversarial_loss = nn.BCELoss()  # Binary Cross-Entropy loss\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))  # Adam optimizer for generator\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))  # Adam optimizer for discriminator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>GAN Training Process Overview</summary>\n",
    "\n",
    "# GAN Training Process Overview\n",
    "\n",
    "## Introduction\n",
    "This code implements a Generative Adversarial Network (GAN) training process.\n",
    "\n",
    "## Model Setup\n",
    "- Uses Binary Cross-Entropy loss (BCELoss) for measuring how well the model is performing\n",
    "- Creates two optimizers using Adam algorithm - one for the Generator and one for the Discriminator\n",
    "- Learning rate is set to 0.0002 with specific beta parameters for stability\n",
    "\n",
    "## Training Process\n",
    "### The train_gan function:\n",
    "- Takes number of epochs and batch size as inputs\n",
    "- Runs in a loop for the specified number of epochs\n",
    "\n",
    "### In each epoch:\n",
    "#### Discriminator Training:\n",
    "- Gets real data samples from dataset\n",
    "- Creates fake data using the Generator\n",
    "- Trains to correctly identify real data as 1 and fake data as 0\n",
    "- Updates Discriminator weights based on its performance\n",
    "\n",
    "#### Generator Training:\n",
    "- Creates fake data\n",
    "- Tries to fool the Discriminator by making fake data look real\n",
    "- Updates Generator weights based on how well it fooled the Discriminator\n",
    "\n",
    "## Monitoring\n",
    "### Progress Tracking:\n",
    "- Every 10 epochs, prints out the losses for both Generator and Discriminator\n",
    "\n",
    "## Analogy\n",
    "Think of it like a game where:\n",
    "- Generator is an art forger trying to create fake paintings\n",
    "- Discriminator is an art expert trying to spot the fakes\n",
    "- They keep getting better at their jobs by competing with each other\n",
    "\n",
    "## Outcome\n",
    "This process helps create a model that can generate realistic-looking synthetic data similar to your original dataset.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(epochs, batch_size):\n",
    "    for epoch in range(epochs):\n",
    "        # Train the Discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Real data - filter only fraud cases\n",
    "        fraud_df = df[df['isFraud'] == 1]\n",
    "        fraud_indices = np.random.randint(0, fraud_df.shape[0], batch_size)\n",
    "        real_data = torch.tensor(fraud_df.values[fraud_indices], dtype=torch.float)\n",
    "        real_labels = torch.ones(batch_size, 1)  # Labels for real data (1)\n",
    "\n",
    "        # Fake data\n",
    "        noise = torch.randn(batch_size, latent_dim)\n",
    "        fake_data = generator(noise)  # Generate fake data using the generator\n",
    "        fake_labels = torch.zeros(batch_size, 1)  # Labels for fake data (0)\n",
    "\n",
    "        # Calculate discriminator loss on real and fake data\n",
    "        real_loss = adversarial_loss(discriminator(real_data), real_labels)\n",
    "        fake_loss = adversarial_loss(discriminator(fake_data.detach()), fake_labels)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        # Backpropagation for discriminator\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Train the Generator\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Generate new fake data and try to fool the discriminator\n",
    "        g_loss = adversarial_loss(discriminator(fake_data), real_labels)\n",
    "\n",
    "        # Backpropagation for generator\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # Print progress every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Discriminator Loss: {d_loss.item()}, Generator Loss: {g_loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the GAN for 1000 epochs (adjust the number of epochs based on your computational resources)\n",
    "train_gan(epochs=1000, batch_size=64)\n",
    "\n",
    "# Generate synthetic data without inverse scaling for now\n",
    "def generate_synthetic_data(generator, num_samples):\n",
    "    noise = torch.randn(num_samples, latent_dim)\n",
    "    synthetic_data = generator(noise).detach().numpy()\n",
    "    return synthetic_data  # Skipping inverse scaling for now\n",
    "\n",
    "# Generate 1000 synthetic samples\n",
    "synthetic_data = generate_synthetic_data(generator, num_samples=1787)\n",
    "print(synthetic_data[:5])  # Display the first 5 synthetic samples\n",
    "\n",
    "# Check the shape\n",
    "print(\"Shape of synthetic data:\", synthetic_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the synthetic data into a DataFrame\n",
    "synthetic_df = pd.DataFrame(synthetic_data, columns=df.columns)\n",
    "\n",
    "# Label the synthetic data as fraudulent\n",
    "synthetic_df['isFraud'] = 1  # Assuming all synthetic data represents fraudulent transactions\n",
    "\n",
    "# Now combine the real and synthetic datasets\n",
    "df = pd.concat([df, synthetic_df], ignore_index=True)\n",
    "\n",
    "# Check the distribution of the target variable to see if the dataset is balanced\n",
    "print(df['isFraud'].value_counts())  # This will show the number of fraudulent and non-fraudulent samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Co-relation Matrix :- \")\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.heatmap(df.corr(), \n",
    "            annot=True, \n",
    "            linewidths=0.9, \n",
    "            fmt=\".1f\", vmin=-1, vmax=1,\n",
    "            cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping unnecessary columns based on correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['isFraud'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['newbalanceOrig', 'oldbalanceDest'], axis=1, inplace=True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolation Forest\n",
    "\n",
    "Isolation Forest is a machine learning algorithm used mainly for anomaly detection (finding unusual or \"outlier\" data points that don't fit well with the rest of the data). It's especially helpful when you want to identify rare, unexpected events—like fraud detection or equipment failure prediction.\n",
    "\n",
    "### How It Works:\n",
    "1. **Random Splits**: The algorithm creates multiple decision trees by randomly splitting the data. Each split separates the data into smaller groups.\n",
    "2. **Isolation of Anomalies**: Outliers (anomalies) are more isolated and separated from the majority of data points, so they need fewer splits to become \"isolated\" in a branch of a tree.\n",
    "3. **Scoring**: The algorithm calculates a score based on how quickly a data point is isolated in a tree. Points that are isolated with fewer splits are considered more likely to be anomalies.\n",
    "\n",
    "### What Is the Anomaly Score?\n",
    "The anomaly score is a value between 0 and 1 that tells us how \"isolated\" a point is compared to the rest of the data. A higher anomaly score means the data point is more likely to be an anomaly (unusual or rare), while a lower score means it's similar to other points in the dataset.\n",
    "\n",
    "### Score Calculation Formula:\n",
    "![Anomaly Score Formula](Images\\anomaly_score.png)\n",
    "\n",
    "#### Where:\n",
    "- `E(h(x))`: The average path length of x across the trees\n",
    "- `c(n)`: It's a scaling factor to make scores easier to interpret, regardless of dataset size.It just adjusts the path length to account for the number of points in your dataset. This way, the anomaly score stays consistent even if you have a very large or small dataset.Without c(n), scores could vary too much with different dataset sizes, making it hard to interpret them reliably.\n",
    "\n",
    "  <details>\n",
    "  <summary>Examples of Anomaly Score Calculation</summary>\n",
    "\n",
    "  Example 1: Small Dataset (10 Points)\n",
    "  Suppose you have a small dataset of 10 points. You want to calculate the anomaly score for a specific point, Point A.\n",
    "\n",
    "  Path Length: Let's say it takes 3 splits on average to isolate Point A in this small dataset.\n",
    "\n",
    "  Calculate c(n):\n",
    "  For a dataset of 10 points, c(10) will be a smaller number—around 4.5 (since there are fewer points to split).\n",
    "\n",
    "  Anomaly Score:\n",
    "  s(x,n) = 2^(-3/4.5) ≈ 2^(-0.67) ≈ 0.63\n",
    "\n",
    "  Example 2: Large Dataset (1000 Points)\n",
    "  Now, let's say you have a much larger dataset with 1000 points and you want to check the same point, Point A.\n",
    "\n",
    "  Path Length: In this large dataset, Point A might take 8 splits on average to be isolated (since there are more points to go through).\n",
    "\n",
    "  Calculate c(n):\n",
    "  For a dataset of 1000 points, c(1000) will be around 14.5 (larger because there are more points).\n",
    "\n",
    "  Anomaly Score:\n",
    "  s(x,n) = 2^(-8/14.5) ≈ 2^(-0.55) ≈ 0.68\n",
    "\n",
    "  Why c(n) Matters:\n",
    "  Without c(n), scores would vary a lot based on the dataset size.\n",
    "  Here, Point A gets similar scores (around 0.63 and 0.68) in both datasets, thanks to c(n).\n",
    "  </details>\n",
    "\n",
    "### Interpreting the Anomaly Score:\n",
    "- Score close to 1: Likely an anomaly\n",
    "- Score close to 0: Likely a normal point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = IsolationForest(contamination=0.05, random_state=42)\n",
    "model.fit(df[['amount', 'oldbalanceOrg']])\n",
    "\n",
    "df['anomaly'] = model.predict(df[['amount', 'oldbalanceOrg']])\n",
    "\n",
    "normal_data = df[df['anomaly'] == 1]\n",
    "fraud_data = df[df['anomaly'] == -1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Scatter Plot<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(normal_data['amount'], normal_data['oldbalanceOrg'], \n",
    "            c='blue', label='Normal', alpha=0.5)\n",
    "plt.scatter(fraud_data['amount'], fraud_data['oldbalanceOrg'], \n",
    "            c='red', label='Fraud', alpha=0.5)\n",
    "plt.title(\"Isolation Forest: Normal vs Fraud Transactions\")\n",
    "plt.xlabel(\"Amount\")\n",
    "plt.ylabel(\"Old Balance Org\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3D Scatter Plot</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(normal_data['amount'], normal_data['oldbalanceOrg'], normal_data['newbalanceDest'], \n",
    "           c='blue', label='Normal', alpha=0.5)\n",
    "ax.scatter(fraud_data['amount'], fraud_data['oldbalanceOrg'], fraud_data['newbalanceDest'], \n",
    "           c='red', label='Fraud', alpha=0.5)\n",
    "\n",
    "ax.set_xlabel('Amount')\n",
    "ax.set_ylabel('Old Balance Org')\n",
    "ax.set_zlabel('New Balance Dest')\n",
    "plt.title(\"3D Scatter Plot: Normal vs Fraud Transactions\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Splitting the data between train and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df[[\"type\", \"amount\", \"oldbalanceOrg\", \"newbalanceDest\"]])\n",
    "y = np.array(df[\"isFraud\"])\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    X, \n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_train shape is ' , x_train.shape)\n",
    "print('X_test shape is ' , x_val.shape)\n",
    "print('y_train shape is ' , y_train.shape)\n",
    "print('y_test shape is ' , y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving Data into s3 Bucket "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "\n",
    "bucket_name = 'anomaly-detection-bucket'\n",
    "\n",
    "\n",
    "train_df = pd.DataFrame(x_train, columns=[\"type\", \"amount\", \"oldbalanceOrg\", \"newbalanceDest\"])\n",
    "train_df['isFraud'] = y_train\n",
    "\n",
    "val_df = pd.DataFrame(x_val, columns=[\"type\", \"amount\", \"oldbalanceOrg\", \"newbalanceDest\"])\n",
    "val_df['isFraud'] = y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('train_data.csv', index=False)\n",
    "val_df.to_csv('val_data.csv', index=False)\n",
    "\n",
    "# Upload to S3\n",
    "s3_client.upload_file(\n",
    "    'train_data.csv', \n",
    "    bucket_name, \n",
    "    'data/train/train_data.csv'\n",
    ")\n",
    "\n",
    "s3_client.upload_file(\n",
    "    'val_data.csv', \n",
    "    bucket_name, \n",
    "    'data/val/val_data.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = 's3://{}/{}'.format(bucket_name, 'data/train')\n",
    "val_data = 's3://{}/{}'.format(bucket_name, 'data/val')\n",
    "\n",
    "train_channel = sagemaker.session.s3_input(train_data,content_type='text/csv')\n",
    "val_channel = sagemaker.session.s3_input(val_data,content_type='text/csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_channels = {\n",
    "    'train': train_channel,\n",
    "    'validation': val_channel\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Cassification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'model/decision_tree_model'\n",
    "s3_output_location = f's3://{bucket_name}/{key}'\n",
    "\n",
    "# Create an SKLearn Estimator for Decision Tree\n",
    "dt_model = SKLearn(\n",
    "    entry_point='train.py',  # Your training script\n",
    "    role=get_execution_role(),  # IAM role for accessing resources\n",
    "    instance_count=1,  # Number of training instances\n",
    "    instance_type='ml.t2.medium',  # Type of instance - free tier eligible\n",
    "    framework_version='0.23-1',  # Version of Scikit-learn\n",
    "    output_path=s3_output_location,  # Location to save model artifacts\n",
    "    sagemaker_session=sagemaker.Session(), # SageMaker session\n",
    "    train_use_spot_instances=True,\n",
    "    train_max_run=300,\n",
    "    train_max_wait=600,\n",
    ")\n",
    "\n",
    "# Set Hyperparameters\n",
    "dt_model.set_hyperparameters(\n",
    "    criterion='gini',  # Splitting criterion: \"gini\" or \"entropy\"\n",
    "    max_depth=5,  # Maximum depth of the tree\n",
    "    min_samples_split=2,  # Minimum samples required to split a node\n",
    "    min_samples_leaf=1,  # Minimum samples required to be a leaf node\n",
    "    random_state=42  # Seed for reproducibility\n",
    ")\n",
    "\n",
    "# Fit the model with the dataset\n",
    "dt_model.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = dt_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.t2.medium'\n",
    ")\n",
    "\n",
    "# Test the endpoint\n",
    "response = predictor.predict([{'feature1': 5, 'feature2': 2, 'feature3': 7}])\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SageMaker endpoint name\n",
    "endpoint_name = \"your-endpoint-name\"\n",
    "\n",
    "# Load validation data (local file or from S3)\n",
    "val_data = pd.read_csv('/path/to/validation_data.csv')  # Replace with your validation data path\n",
    "X_val = val_data.drop('target', axis=1)  # Replace 'target' with your actual target column name\n",
    "y_val = val_data['target']\n",
    "\n",
    "# Initialize the SageMaker runtime client\n",
    "runtime_client = boto3.client('sagemaker-runtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_from_endpoint(endpoint_name, data):\n",
    "    predictions = []\n",
    "    for _, row in data.iterrows():\n",
    "        payload = row.values.tolist()\n",
    "        response = runtime_client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType=\"text/csv\",  # Ensure this matches your endpoint's expected input\n",
    "            Body=\",\".join(map(str, payload))\n",
    "        )\n",
    "        predictions.append(float(response['Body'].read().decode('utf-8')))\n",
    "    return np.array(predictions)\n",
    "\n",
    "# Get predictions\n",
    "y_pred = get_predictions_from_endpoint(endpoint_name, X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_val, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_val), yticklabels=np.unique(y_val))\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting some new values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets predict the transactions\n",
    "features = np.array([[4, 9000.60, 9000.60, 0.00]])\n",
    "print(model.predict(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array([[2, 9839.64, 170136.00, 160296.36]])\n",
    "print(model.predict(features))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
